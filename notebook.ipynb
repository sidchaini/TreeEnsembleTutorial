{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN+rlnUg0gWTDhEhPjV5Uph"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![github-badge](https://img.shields.io/badge/GitHub-sidchaini/TreeEnsembleTutorial-blue)](https://github.com/sidchaini/TreeEnsembleTutorial)\n",
        "\n",
        "**Siddharth Chaini, 28th October, 2025**\n",
        "\n",
        "(Thanks to Federica Bianco, Ashish Mahabal and Ajit Kembhavi)"
      ],
      "metadata": {
        "id": "0Tu9D9_4DNvp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classifying Variable Stars from the Zwicky Transient Facility\n",
        "\n",
        "For this example, we will be using data from a paper of mine: [![arxiv-badge](https://img.shields.io/badge/arXiv-2403.12120-red)](https://arxiv.org/abs/2403.12120)\n",
        "\n",
        "The paper is about classifying variable stars, but for this tutorial, I created a sample version of this consisting of 4 classes of stars (500 objects from each class).\n",
        "\n",
        "We'll look at how to apply a random forest classifier as well as a boosted tree classifier."
      ],
      "metadata": {
        "id": "QWW5sdJID5p3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Some Scientific Intro\n",
        "\n",
        "The 4 classes stars are:\n",
        "1. [Cepheid Variables](https://en.wikipedia.org/wiki/Cepheid_variable) (```CEP```)\n",
        "2. [Delta Scuti Variables](https://en.wikipedia.org/wiki/Delta_Scuti_variable) (```DSCT```)\n",
        "3. [Mira Variables](https://en.wikipedia.org/wiki/Mira_variable) (```Mira```)\n",
        "4. [RR Lyrae Variables](https://en.wikipedia.org/wiki/RR_Lyrae_variable) (```RR```)\n",
        "\n",
        "If you are interested in them, you can read more about them on their wikipedia pages (linked above).\n",
        "\n",
        "These are all variable stars: meaning, that their brightness changes over time.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/sidchaini/TreeEnsembleTutorial/refs/heads/main/animation.gif\" width=\"400\">\n",
        "\n",
        "Each of them have some features to separate them. A plot of brightness over time is called a \"light curve\". From these light curves, we can extract summary features.\n",
        "\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/sidchaini/TreeEnsembleTutorial/refs/heads/main/lightcurve.png\" width=\"400\">\n",
        "\n",
        "We'll be looking at:\n",
        "1) **Period**: The time it takes for the star's brightness to complete one full cycle.\n",
        "2) **Amplitude**: The range of brightness variation (max brightness - min brightness).\n",
        "3) **Mean brightness**: The average brightness of the star.\n",
        "4) **Std dev in brightness**: The standard deviation of the brightness measurements.\n"
      ],
      "metadata": {
        "id": "Mt9tUp34Eo2x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Data Preparation\n",
        "\n",
        "### 1.1. Loading the Data"
      ],
      "metadata": {
        "id": "bnzf4tKpF8yU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "NaWqVRLRD6NT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"https://raw.githubusercontent.com/sidchaini/TreeEnsembleTutorial/refs/heads/main/variable_stars_ztf.csv\", index_col=0)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "eEGtbMzHF572"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"class\"].value_counts()"
      ],
      "metadata": {
        "id": "5cozsKSPLEam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_names = [\"Period_band_r\", \"Amplitude_r\", \"Mean_r\", \"Std_r\"]\n",
        "X = df[feature_names].to_numpy()\n",
        "y = df[\"class\"].to_numpy()"
      ],
      "metadata": {
        "id": "qmNpv2GrGGUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2. Split into train-test"
      ],
      "metadata": {
        "id": "TF6pzY0MHIeW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "print(\"Shape of training features:\", X_train.shape)\n",
        "print(\"Shape of testing features:\", X_test.shape)\n",
        "print(\"Shape of training labels:\", y_train.shape)\n",
        "print(\"Shape of testing labels:\", y_test.shape)"
      ],
      "metadata": {
        "id": "jUr1jWCYHKBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Random Forest\n",
        "\n",
        "A Random Forest is an ensemble learning method that operates by constructing a many decision trees at training time, which are based on bagging. For a classification task, the final prediction is the class selected by most trees.\n",
        "\n",
        "Two important things to consider,\n",
        "\n",
        "- `max_depth`: This limits the maximum depth of each decision tree. A smaller depth can prevent overfitting by making the model less complex.\n",
        "- `random_state` (seed): This ensures that the randomness involved in building the forest (like bootstrapping samples and selecting features) is the same every time, making our model's results reproducible.\n",
        "\n",
        "\n",
        "---\n",
        "NOTE: The syntax for regression is very similar. You would just import and use `RandomForestRegressor` instead of `RandomForestClassifier`.\n"
      ],
      "metadata": {
        "id": "vWQV5nvTHLHp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1. Instantiate and Train"
      ],
      "metadata": {
        "id": "8u3_d97fHWVY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    # max_depth=2,\n",
        "    n_jobs=-1, random_state=44)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "print(f\"Train Score: {rf.score(X_train, y_train):.2%}\")\n",
        "print(f\"Test Score: {rf.score(X_test, y_test):.2%}\")"
      ],
      "metadata": {
        "id": "Zz4kSBoMLgej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2. Evaluate performance"
      ],
      "metadata": {
        "id": "IqP5EflqHaeC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay, roc_curve, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ConfusionMatrixDisplay.from_estimator(rf, X_test, y_test, cmap=plt.cm.Blues, ax=ax)\n",
        "ax.set_title(\"Confusion Matrix for Random Forest\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "02n-BfkqLnUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_roc_curve(y_test, rf):\n",
        "  y_test_binarized = label_binarize(y_test, classes=rf.classes_)\n",
        "  n_classes = y_test_binarized.shape[1]\n",
        "  y_prob_rf = rf.predict_proba(X_test)\n",
        "\n",
        "  fpr = dict()\n",
        "  tpr = dict()\n",
        "  roc_auc = dict()\n",
        "  for i in range(n_classes):\n",
        "      fpr[i], tpr[i], _ = roc_curve(y_test_binarized[:, i], y_prob_rf[:, i])\n",
        "      roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "  plt.figure()\n",
        "  for i in range(n_classes):\n",
        "      plt.plot(fpr[i], tpr[i], lw=2,\n",
        "              label='{0} (area = {1:0.2f})'\n",
        "              ''.format(rf.classes_[i], roc_auc[i]))\n",
        "\n",
        "  plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "  plt.xlim([0.0, 1.0])\n",
        "  plt.ylim([0.0, 1.05])\n",
        "  plt.xlabel('False Positive Rate')\n",
        "  plt.ylabel('True Positive Rate')\n",
        "  plt.title('Multi-class ROC for Random Forest')\n",
        "  plt.legend(loc=\"lower right\")\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "Ep8gKnYcMGTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_roc_curve(y_test, rf)"
      ],
      "metadata": {
        "id": "Mj-rqEIvMFsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. GradientBoostingClassifier\n",
        "A Gradient Boosting builds trees sequentially, and every new tree corrects the errors of the previous ones.\n",
        "\n",
        "NOTE: For regression, the syntax is again very analogous: you would use `GradientBoostingRegressor`.\n",
        "\n",
        "Also, another very popular implementation of gradient boosting is XGBoost. and is very similar: `from xgboost import XGBClassifier`.\n",
        "\n",
        "But we'll use `sklearn` for this tutorial.\n"
      ],
      "metadata": {
        "id": "_GL9-oj2Hoq7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1. Instantiate and Train"
      ],
      "metadata": {
        "id": "tGYOQrjDNXcM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "gbt = GradientBoostingClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=1,\n",
        "    random_state=44)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "gbt.fit(X_train, y_train)\n",
        "\n",
        "print(f\"Train Score: {gbt.score(X_train, y_train):.2%}\")\n",
        "print(f\"Test Score: {gbt.score(X_test, y_test):.2%}\")"
      ],
      "metadata": {
        "id": "hSlzzUKDHrHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Evaluate performance"
      ],
      "metadata": {
        "id": "oysZmeffNZRU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "ConfusionMatrixDisplay.from_estimator(gbt, X_test, y_test, cmap=plt.cm.Blues, ax=ax)\n",
        "ax.set_title(\"Confusion Matrix for Gradient Boosting\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zAhNv-SCNVs3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_roc_curve(y_test, gbt)"
      ],
      "metadata": {
        "id": "IkgU4io_NeDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Feature importance"
      ],
      "metadata": {
        "id": "bS56U5mXH9Zw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fistd = np.std([tree.feature_importances_ for tree in rf.estimators_], axis=0)\n",
        "importances_rf = rf.feature_importances_\n",
        "\n",
        "print(\"Random Forest feature importance:\")\n",
        "for f, fi, s in zip(feature_names, importances_rf, fistd):\n",
        "  print(f, fi.round(3), \"+/-\", s.round(3))\n",
        "\n",
        "indices_rf = np.argsort(importances_rf)\n",
        "\n",
        "plt.figure()\n",
        "plt.title(\"Feature importances (Random Forest)\")\n",
        "plt.barh(range(len(indices_rf)), importances_rf[indices_rf],\n",
        "         xerr=fistd[indices_rf], align=\"center\")\n",
        "plt.yticks(range(len(indices_rf)), [feature_names[i] for i in indices_rf])\n",
        "plt.xlabel(\"Importance\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IXP78mlnI-OF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importances_gbt = gbt.feature_importances_\n",
        "indices_gbt = np.argsort(importances_gbt)\n",
        "\n",
        "print(\"\\nGradient Boosting feature importance:\")\n",
        "for f, fi in zip(feature_names, importances_gbt):\n",
        "  print(f, fi.round(3))\n",
        "\n",
        "\n",
        "plt.figure()\n",
        "plt.title(\"Feature importances (Gradient Boosting)\")\n",
        "plt.barh(range(len(indices_gbt)), importances_gbt[indices_gbt], align=\"center\")\n",
        "plt.yticks(range(len(indices_gbt)), [feature_names[i] for i in indices_gbt])\n",
        "plt.xlabel(\"Importance\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wqNGMuf1Nmv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uflUfFFINrYF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}